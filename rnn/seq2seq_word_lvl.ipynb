{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "seq2seq_word_lvl.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXxd0SFVykk2",
        "outputId": "48f6ebc5-2272-4655-8f2a-90f986955b1b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXTO1Y6Nx-Bv"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvK346_ix-B1"
      },
      "source": [
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 50  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on."
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwrKXKaux-B2"
      },
      "source": [
        "lines = pd.read_table('/content/drive/MyDrive/Colab Notebooks/deep_learning_studia/rnn/pol.txt', names=['eng', 'pol', 'contributor'])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "GfQiejZMx-B2",
        "outputId": "31cd641c-43fe-4af8-dcf1-ac1bee4cd015"
      },
      "source": [
        "lines"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>pol</th>\n",
              "      <th>contributor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Idź.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Cześć.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Uciekaj!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run.</td>\n",
              "      <td>Biegnij.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run.</td>\n",
              "      <td>Uciekaj.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40460</th>\n",
              "      <td>No matter how much you try to convince people ...</td>\n",
              "      <td>Nieważne, jak bardzo usiłujesz przekonać ludzi...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40461</th>\n",
              "      <td>A child who is a native speaker usually knows ...</td>\n",
              "      <td>Dziecko zwykle wie o swoim języku ojczystym rz...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40462</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Zwykle jest wiele stron internetowych na każdy...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40463</th>\n",
              "      <td>If you want to sound like a native speaker, yo...</td>\n",
              "      <td>Jeśli chcesz mówić jak rodzimy użytkownik, mus...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40464</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Jeśli ktoś, kto nas nie zna, mówi, że mówimy j...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40465 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     eng  ...                                        contributor\n",
              "0                                                    Go.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "1                                                    Hi.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
              "2                                                   Run!  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "3                                                   Run.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "4                                                   Run.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "...                                                  ...  ...                                                ...\n",
              "40460  No matter how much you try to convince people ...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "40461  A child who is a native speaker usually knows ...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "40462  Since there are usually multiple websites on a...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "40463  If you want to sound like a native speaker, yo...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "40464  If someone who doesn't know your background sa...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "\n",
              "[40465 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGBY1_Wpx-B3"
      },
      "source": [
        "lines = lines.drop(columns=['contributor'])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2tb3heFx-B4"
      },
      "source": [
        "lines = lines[0:10000]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "4vQ-7JFmx-B4",
        "outputId": "a10009e9-68f9-4413-b5f2-9516b404b3be"
      },
      "source": [
        "lines"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>pol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Idź.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Cześć.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Uciekaj!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run.</td>\n",
              "      <td>Biegnij.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run.</td>\n",
              "      <td>Uciekaj.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>Where is your school?</td>\n",
              "      <td>Gdzie jest twoja szkoła?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>Where's my breakfast?</td>\n",
              "      <td>Gdzie jest moje śniadanie?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Where's the bathroom?</td>\n",
              "      <td>Gdzie jest toaleta?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Where's the bus stop?</td>\n",
              "      <td>Gdzie znajduje się przystanek autobusowy?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>Where's the bus stop?</td>\n",
              "      <td>Gdzie jest przystanek?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        eng                                        pol\n",
              "0                       Go.                                       Idź.\n",
              "1                       Hi.                                     Cześć.\n",
              "2                      Run!                                   Uciekaj!\n",
              "3                      Run.                                   Biegnij.\n",
              "4                      Run.                                   Uciekaj.\n",
              "...                     ...                                        ...\n",
              "9995  Where is your school?                   Gdzie jest twoja szkoła?\n",
              "9996  Where's my breakfast?                 Gdzie jest moje śniadanie?\n",
              "9997  Where's the bathroom?                        Gdzie jest toaleta?\n",
              "9998  Where's the bus stop?  Gdzie znajduje się przystanek autobusowy?\n",
              "9999  Where's the bus stop?                     Gdzie jest przystanek?\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJDlfuxx-B5"
      },
      "source": [
        "# Lowercase characters\n",
        "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
        "lines.pol=lines.pol.apply(lambda x: x.lower())"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QeFsHm7x-B5"
      },
      "source": [
        "# Take the length as 50\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
        "lines.pol=lines.pol.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze9-YOeVx-B5"
      },
      "source": [
        "exclude = set(string.punctuation)\n",
        "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines.pol=lines.pol.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-s5u9pmx-B5"
      },
      "source": [
        "# Remove digits\n",
        "remove_digits = str.maketrans('', '', string.digits)\n",
        "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
        "lines.pol=lines.pol.apply(lambda x: x.translate(remove_digits))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nEiKPFUQx-B6",
        "outputId": "e36ccb97-c3f4-4b77-f9aa-aff9bc4f11cd"
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>pol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>go</td>\n",
              "      <td>idź</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hi</td>\n",
              "      <td>cześć</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>run</td>\n",
              "      <td>uciekaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>run</td>\n",
              "      <td>biegnij</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>run</td>\n",
              "      <td>uciekaj</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   eng      pol\n",
              "0   go      idź\n",
              "1   hi    cześć\n",
              "2  run  uciekaj\n",
              "3  run  biegnij\n",
              "4  run  uciekaj"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-vfUTgx-B6"
      },
      "source": [
        "# Add start and end tokens to target sequences\n",
        "lines.pol = lines.pol.apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNS83yyx-B6"
      },
      "source": [
        "# Vocabulary of English\n",
        "all_eng_words=set()\n",
        "for eng in lines.eng:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10BaQguZx-B7"
      },
      "source": [
        "# Vocabulary of Polish\n",
        "all_pol_words=set()\n",
        "for pol in lines.pol:\n",
        "    for word in pol.split():\n",
        "        if word not in all_pol_words:\n",
        "            all_pol_words.add(word)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeeoL6NTx-B7",
        "outputId": "cf039e0b-374c-44d2-c5bf-638331bc2e46"
      },
      "source": [
        "len(all_eng_words), len(all_pol_words)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3119, 6139)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBJk4RVx-B7"
      },
      "source": [
        "# Max Length of source sequence\n",
        "lenght_list=[]\n",
        "for l in lines.eng:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_eng = np.max(lenght_list)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBBoyWXIx-B7"
      },
      "source": [
        "# Max Length of target sequence\n",
        "lenght_list=[]\n",
        "for l in lines.pol:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_pol = np.max(lenght_list)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiNNOlD6x-B7"
      },
      "source": [
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_pol_words))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fayj_5pKx-B8"
      },
      "source": [
        "# Calculate Vocab size for both source and target\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_pol_words)\n",
        "num_decoder_tokens += 1 # For zero padding"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEzpORnXx-B8"
      },
      "source": [
        "# Create word to token dictionary for both source and target\n",
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aREkrljix-B8",
        "outputId": "f006bab1-9c39-42a2-8a16-e1504ef82c30"
      },
      "source": [
        "len(lines.pol)*16*num_decoder_tokens"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "982400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "2Ca-paQOx-B8",
        "outputId": "e659a830-560b-4dc1-e116-34d36d2cbc3b"
      },
      "source": [
        "lines.sample(10)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>pol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1850</th>\n",
              "      <td>bring it back</td>\n",
              "      <td>START_ oddaj to _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5032</th>\n",
              "      <td>you were invited</td>\n",
              "      <td>START_ zostałeś zaproszony _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9722</th>\n",
              "      <td>time was running out</td>\n",
              "      <td>START_ czas się kończył _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>is that snow</td>\n",
              "      <td>START_ czy to jest śnieg _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3306</th>\n",
              "      <td>how is your dad</td>\n",
              "      <td>START_ jak się miewa twój tata _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>tom cheered</td>\n",
              "      <td>START_ tom wiwatował _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8629</th>\n",
              "      <td>toms eyes were red</td>\n",
              "      <td>START_ tom miał zaczerwienione oczy _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>i smoke</td>\n",
              "      <td>START_ palę _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4940</th>\n",
              "      <td>what did she say</td>\n",
              "      <td>START_ co ona powiedziała _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4459</th>\n",
              "      <td>i prefer reading</td>\n",
              "      <td>START_ wolę czytać _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       eng                                       pol\n",
              "1850         bring it back                      START_ oddaj to _END\n",
              "5032      you were invited           START_ zostałeś zaproszony _END\n",
              "9722  time was running out              START_ czas się kończył _END\n",
              "1529          is that snow             START_ czy to jest śnieg _END\n",
              "3306       how is your dad       START_ jak się miewa twój tata _END\n",
              "1210           tom cheered                 START_ tom wiwatował _END\n",
              "8629    toms eyes were red  START_ tom miał zaczerwienione oczy _END\n",
              "124                i smoke                          START_ palę _END\n",
              "4940      what did she say            START_ co ona powiedziała _END\n",
              "4459      i prefer reading                   START_ wolę czytać _END"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eijiCiWx-B9"
      },
      "source": [
        "encoder_input_data = np.zeros((len(lines.eng), 7), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(lines.pol), 16), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(lines.pol), 16, num_decoder_tokens), dtype='float32')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cCpKrdRx-B9"
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.pol)):\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "        encoder_input_data[i, t] = input_token_index[word]\n",
        "    for t, word in enumerate(target_text.split()):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = target_token_index[word]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtr5C-AmpPK6",
        "outputId": "72acec67-415c-442c-d943-83391582be31"
      },
      "source": [
        "encoder_input_data"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1149.,    0.,    0., ...,    0.,    0.,    0.],\n",
              "       [1276.,    0.,    0., ...,    0.,    0.,    0.],\n",
              "       [2258.,    0.,    0., ...,    0.,    0.,    0.],\n",
              "       ...,\n",
              "       [3017., 2721.,  235., ...,    0.,    0.,    0.],\n",
              "       [3017., 2721.,  390., ...,    0.,    0.,    0.],\n",
              "       [3017., 2721.,  390., ...,    0.,    0.,    0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vv80V17x-B9"
      },
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens+1, latent_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens+1, latent_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Use a softmax to generate a probability distribution over the target vocabulary for each time step\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turnA\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-hpMtSOx-B9",
        "outputId": "1d06b86b-5c7d-486b-9b8f-eb2b064a81a1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_35 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_36 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, None, 50)     156000      input_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, None, 50)     307050      input_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_22 (LSTM)                  [(None, 50), (None,  20200       embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_23 (LSTM)                  [(None, None, 50), ( 20200       embedding_23[0][0]               \n",
            "                                                                 lstm_22[0][1]                    \n",
            "                                                                 lstm_22[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, None, 6140)   313140      lstm_23[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 816,590\n",
            "Trainable params: 816,590\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4iN0MHfx-B-",
        "outputId": "719c18a3-92ed-47eb-b9ae-0d81f52801ff"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 42s 120ms/step - loss: 1.6476 - acc: 0.1966 - val_loss: 1.7244 - val_acc: 0.1780\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.3104 - acc: 0.2096 - val_loss: 1.6904 - val_acc: 0.2054\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.2290 - acc: 0.2290 - val_loss: 1.6492 - val_acc: 0.2184\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 1.1858 - acc: 0.2393 - val_loss: 1.6171 - val_acc: 0.2217\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.1464 - acc: 0.2515 - val_loss: 1.5937 - val_acc: 0.2360\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 1.1159 - acc: 0.2649 - val_loss: 1.5688 - val_acc: 0.2459\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 1.0817 - acc: 0.2770 - val_loss: 1.5462 - val_acc: 0.2492\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 1.0707 - acc: 0.2876 - val_loss: 1.5405 - val_acc: 0.2604\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 1.0318 - acc: 0.2975 - val_loss: 1.5265 - val_acc: 0.2671\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.0039 - acc: 0.3093 - val_loss: 1.5043 - val_acc: 0.2732\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 0.9825 - acc: 0.3140 - val_loss: 1.5020 - val_acc: 0.2793\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.9692 - acc: 0.3199 - val_loss: 1.4937 - val_acc: 0.2850\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.9441 - acc: 0.3287 - val_loss: 1.4871 - val_acc: 0.2865\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 0.9247 - acc: 0.3344 - val_loss: 1.4877 - val_acc: 0.2939\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 0.9074 - acc: 0.3408 - val_loss: 1.4860 - val_acc: 0.2952\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.8874 - acc: 0.3457 - val_loss: 1.4765 - val_acc: 0.2982\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.8826 - acc: 0.3509 - val_loss: 1.4808 - val_acc: 0.3018\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.8680 - acc: 0.3588 - val_loss: 1.4632 - val_acc: 0.3049\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.8537 - acc: 0.3605 - val_loss: 1.4733 - val_acc: 0.3066\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.8415 - acc: 0.3655 - val_loss: 1.4651 - val_acc: 0.3081\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.8224 - acc: 0.3666 - val_loss: 1.4573 - val_acc: 0.3094\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.8026 - acc: 0.3757 - val_loss: 1.4649 - val_acc: 0.3085\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7885 - acc: 0.3794 - val_loss: 1.4632 - val_acc: 0.3137\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7801 - acc: 0.3828 - val_loss: 1.4565 - val_acc: 0.3146\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7605 - acc: 0.3900 - val_loss: 1.4528 - val_acc: 0.3121\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7507 - acc: 0.3901 - val_loss: 1.4658 - val_acc: 0.3159\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7333 - acc: 0.3986 - val_loss: 1.4626 - val_acc: 0.3162\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.7352 - acc: 0.4004 - val_loss: 1.4721 - val_acc: 0.3153\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7126 - acc: 0.4053 - val_loss: 1.4855 - val_acc: 0.3152\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.7106 - acc: 0.4057 - val_loss: 1.4815 - val_acc: 0.3173\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.6985 - acc: 0.4134 - val_loss: 1.4798 - val_acc: 0.3171\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.6812 - acc: 0.4162 - val_loss: 1.4831 - val_acc: 0.3184\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.6752 - acc: 0.4221 - val_loss: 1.4934 - val_acc: 0.3195\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.6666 - acc: 0.4240 - val_loss: 1.5005 - val_acc: 0.3184\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.6524 - acc: 0.4288 - val_loss: 1.4949 - val_acc: 0.3144\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.6462 - acc: 0.4337 - val_loss: 1.4996 - val_acc: 0.3215\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.6359 - acc: 0.4396 - val_loss: 1.5049 - val_acc: 0.3230\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.6244 - acc: 0.4433 - val_loss: 1.5113 - val_acc: 0.3208\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.6162 - acc: 0.4466 - val_loss: 1.5274 - val_acc: 0.3216\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.5994 - acc: 0.4514 - val_loss: 1.5263 - val_acc: 0.3211\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.5966 - acc: 0.4537 - val_loss: 1.5365 - val_acc: 0.3206\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.5830 - acc: 0.4570 - val_loss: 1.5419 - val_acc: 0.3207\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 0.5736 - acc: 0.4609 - val_loss: 1.5557 - val_acc: 0.3203\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.5671 - acc: 0.4685 - val_loss: 1.5629 - val_acc: 0.3194\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.5570 - acc: 0.4696 - val_loss: 1.5706 - val_acc: 0.3214\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.5488 - acc: 0.4748 - val_loss: 1.5755 - val_acc: 0.3193\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.5394 - acc: 0.4787 - val_loss: 1.5822 - val_acc: 0.3221\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.5239 - acc: 0.4820 - val_loss: 1.5935 - val_acc: 0.3178\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.5267 - acc: 0.4816 - val_loss: 1.6056 - val_acc: 0.3187\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.5110 - acc: 0.4871 - val_loss: 1.6139 - val_acc: 0.3130\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.5067 - acc: 0.4920 - val_loss: 1.6230 - val_acc: 0.3146\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 0.4980 - acc: 0.4975 - val_loss: 1.6397 - val_acc: 0.3151\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.4884 - acc: 0.5024 - val_loss: 1.6484 - val_acc: 0.3113\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.4797 - acc: 0.5062 - val_loss: 1.6553 - val_acc: 0.3158\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 26s 106ms/step - loss: 0.4733 - acc: 0.5083 - val_loss: 1.6699 - val_acc: 0.3092\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.4609 - acc: 0.5122 - val_loss: 1.6825 - val_acc: 0.3121\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.4543 - acc: 0.5162 - val_loss: 1.7010 - val_acc: 0.3089\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.4464 - acc: 0.5202 - val_loss: 1.7147 - val_acc: 0.3092\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.4407 - acc: 0.5225 - val_loss: 1.7300 - val_acc: 0.3087\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.4345 - acc: 0.5267 - val_loss: 1.7438 - val_acc: 0.3105\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.4314 - acc: 0.5264 - val_loss: 1.7622 - val_acc: 0.3065\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.4195 - acc: 0.5364 - val_loss: 1.7752 - val_acc: 0.3034\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.4177 - acc: 0.5357 - val_loss: 1.7841 - val_acc: 0.3065\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.4054 - acc: 0.5408 - val_loss: 1.8125 - val_acc: 0.2956\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3945 - acc: 0.5469 - val_loss: 1.8097 - val_acc: 0.3066\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3880 - acc: 0.5507 - val_loss: 1.8307 - val_acc: 0.3018\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 27s 108ms/step - loss: 0.3846 - acc: 0.5492 - val_loss: 1.8447 - val_acc: 0.3038\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.3785 - acc: 0.5539 - val_loss: 1.8525 - val_acc: 0.3021\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.3752 - acc: 0.5569 - val_loss: 1.8651 - val_acc: 0.2967\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3673 - acc: 0.5634 - val_loss: 1.8696 - val_acc: 0.3002\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3570 - acc: 0.5676 - val_loss: 1.8872 - val_acc: 0.3000\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3505 - acc: 0.5694 - val_loss: 1.9049 - val_acc: 0.2953\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3435 - acc: 0.5693 - val_loss: 1.9152 - val_acc: 0.2935\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3468 - acc: 0.5711 - val_loss: 1.9300 - val_acc: 0.2934\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.3369 - acc: 0.5750 - val_loss: 1.9475 - val_acc: 0.2889\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.3278 - acc: 0.5828 - val_loss: 1.9703 - val_acc: 0.2911\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.3241 - acc: 0.5852 - val_loss: 1.9779 - val_acc: 0.2941\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.3136 - acc: 0.5884 - val_loss: 1.9916 - val_acc: 0.2905\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 27s 109ms/step - loss: 0.3093 - acc: 0.5917 - val_loss: 2.0167 - val_acc: 0.2887\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.3007 - acc: 0.5965 - val_loss: 2.0389 - val_acc: 0.2856\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2991 - acc: 0.5960 - val_loss: 2.0479 - val_acc: 0.2876\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2967 - acc: 0.5997 - val_loss: 2.0569 - val_acc: 0.2888\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.2896 - acc: 0.6034 - val_loss: 2.0903 - val_acc: 0.2841\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.2850 - acc: 0.6029 - val_loss: 2.0952 - val_acc: 0.2874\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2787 - acc: 0.6082 - val_loss: 2.1270 - val_acc: 0.2784\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2771 - acc: 0.6093 - val_loss: 2.1333 - val_acc: 0.2804\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2727 - acc: 0.6145 - val_loss: 2.1716 - val_acc: 0.2731\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2647 - acc: 0.6149 - val_loss: 2.1692 - val_acc: 0.2818\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 0.2609 - acc: 0.6213 - val_loss: 2.1976 - val_acc: 0.2772\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 0.2581 - acc: 0.6219 - val_loss: 2.1967 - val_acc: 0.2799\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 27s 110ms/step - loss: 0.2531 - acc: 0.6254 - val_loss: 2.2040 - val_acc: 0.2834\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2447 - acc: 0.6266 - val_loss: 2.2424 - val_acc: 0.2741\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2455 - acc: 0.6264 - val_loss: 2.2507 - val_acc: 0.2777\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 0.2425 - acc: 0.6302 - val_loss: 2.2656 - val_acc: 0.2753\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2369 - acc: 0.6326 - val_loss: 2.3008 - val_acc: 0.2681\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 0.2247 - acc: 0.6371 - val_loss: 2.3048 - val_acc: 0.2728\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2265 - acc: 0.6370 - val_loss: 2.3063 - val_acc: 0.2733\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2266 - acc: 0.6373 - val_loss: 2.3321 - val_acc: 0.2725\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 0.2190 - acc: 0.6416 - val_loss: 2.3340 - val_acc: 0.2727\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 27s 107ms/step - loss: 0.2154 - acc: 0.6423 - val_loss: 2.3554 - val_acc: 0.2732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f83d0d0f668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDh_bH74S-dn",
        "outputId": "f7b676ec-617b-4752-ee6b-92c1857ed169"
      },
      "source": [
        "# define the encoder model \r\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\r\n",
        "encoder_model.summary()\r\n",
        "# Redefine the decoder model with decoder will be getting below inputs from encoder while in prediction\r\n",
        "decoder_state_input_h = Input(shape=(50,))\r\n",
        "decoder_state_input_c = Input(shape=(50,))\r\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n",
        "final_dex2= dex(decoder_inputs)\r\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\r\n",
        "decoder_states2 = [state_h2, state_c2]\r\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\r\n",
        "# sampling model will take encoder states and decoder_input(seed initially) and output the predictions(french word index) We dont care about decoder_states2\r\n",
        "decoder_model = Model(\r\n",
        "    [decoder_inputs] + decoder_states_inputs,\r\n",
        "    [decoder_outputs2] + decoder_states2)\r\n",
        "# Reverse-lookup token index to decode sequences back to\r\n",
        "# something readable.\r\n",
        "reverse_input_char_index = dict(\r\n",
        "    (i, char) for char, i in input_token_index.items())\r\n",
        "reverse_target_char_index = dict(\r\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_35 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_22 (Embedding)     (None, None, 50)          156000    \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               [(None, 50), (None, 50),  20200     \n",
            "=================================================================\n",
            "Total params: 176,200\n",
            "Trainable params: 176,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sec0JML4x-B-"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "# Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "# Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 52):\n",
        "            stop_condition = True\n",
        "# Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "# Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW04FA6gx-B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebe2ee4-91cf-498b-c229-2879897ba2d4"
      },
      "source": [
        "for seq_index in [1414, 304, 4231, 8506, 7348, 6789, 5678]:\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', lines.eng[seq_index: seq_index + 1])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: 1414    have courage\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  poddawaj piłkę piłkę czytać iść pani _END\n",
            "-\n",
            "Input sentence: 304    try some\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  moim to się pan do do do do do do do do do do do do do\n",
            "-\n",
            "Input sentence: 4231    everyone cheered\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  okazji powietrza umrzeć pani pani _END\n",
            "-\n",
            "Input sentence: 8506    tom has a chauffeur\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  łódkę życie jak iść pani _END\n",
            "-\n",
            "Input sentence: 7348    were closed today\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  dwójka spójrz list iść iść do _END\n",
            "-\n",
            "Input sentence: 6789    is that your house\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  twoim twoim mało mało lunch _END\n",
            "-\n",
            "Input sentence: 5678    she isnt married\n",
            "Name: eng, dtype: object\n",
            "Decoded sentence:  nie ma ma to się do _END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYV-zBm9mf-Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}